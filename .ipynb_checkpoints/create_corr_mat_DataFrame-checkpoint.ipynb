{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add correlation matrices to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load *df_parameter* DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par = pd.read_pickle('df_parameter.p')\n",
    "display(df_par.head())\n",
    "df_par.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.loc[:, 'gBB'].unique()\n",
    "df_par.loc[:, 'gBC'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 200  # size of grid\n",
    "\n",
    "# create labels for columns, these correspond to the upper triangular of a matrix\n",
    "cols_x1x2 = [f\"x{j+1}_x{i+1}\" for i in range(n_grid) for j in range(i+1)]\n",
    "\n",
    "print(cols_x1x2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load npz-files and add pixels as features\n",
    "\n",
    "1. Create new dataframe with flattened npz-matrices as rows and x1-x2 values as columns\n",
    "2. Concat parameter df and correlation-matrix df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt: Reading all at once (not working)\n",
    "\n",
    "Load npz-files and add pixels as features\n",
    "\n",
    "1. Create new dataframe with flattened npz-matrices as rows and x1-x2 values as columns\n",
    "2. Concat parameter df and correlation-matrix df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_list = []\n",
    "for i, path_npz in enumerate(df_par.loc[:, 'path']):\n",
    "    \n",
    "    corrBC = np.load(path_npz + '/correlation_fct_BC.npz')['corrBC']\n",
    "    corrBC_flat = corrBC.flatten().astype(np.float32)  # reduce precision to improve performance\n",
    "    corr_list.append(corrBC_flat)\n",
    "\n",
    "df_corr = pd.DataFrame(corr_list, columns=cols_x1x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attempt: Store as pickles and concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load npz files and change to float32.\n",
    "2. Correlation functions are symmetric, therefore, store only upper triangular.\n",
    "3. Flatten correlation matrices, they denote the rows of the DataFrames.\n",
    "4. Store DataFrames in chunks of 1000 as pickles.\n",
    "5. Restart notebook to drop loaded memory.\n",
    "6. Reload DataFrames and concat them to a big DataFrame.\n",
    "7. Store it as a pickle.\n",
    "\n",
    "1\\. First check if storing the matrices with float16-type is still acceptable or we should go with float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gBB, gCC, gAB, gAC, gBC = 1.0, 1.0, -1.0, 0.2, 0.2\n",
    "mask = (\n",
    "    (df_par.loc[:, 'gBB'] == gBB) &\n",
    "    (df_par.loc[:, 'gCC'] == gCC) &\n",
    "    (df_par.loc[:, 'gAB'] == gAB) &\n",
    "    (df_par.loc[:, 'gAC'] == gAC) &\n",
    "    (df_par.loc[:, 'gBC'] == gBC)\n",
    ")\n",
    "corrBC_example = np.load(df_par.loc[mask, 'path'].iloc[0] + '/correlation_fct_BC.npz')['corrBC']\n",
    "\n",
    "\n",
    "\n",
    "xgrid = np.linspace(-2.5, 2.5, 200)\n",
    "x, y = np.meshgrid(xgrid, xgrid)\n",
    "corrBC_float64 = np.array(corrBC_example, dtype=np.float64)\n",
    "corrBC_float32 = np.array(corrBC_example, dtype=np.float32)\n",
    "corrBC_float16 = np.array(corrBC_example, dtype=np.float16)\n",
    "\n",
    "print(np.abs(corrBC_float64 - corrBC_float32).max())\n",
    "print(np.abs(corrBC_float64 - corrBC_float16).max())\n",
    "print(np.abs(corrBC_float32 - corrBC_float16).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float32 and float16 seems to be fine. However, continue with float32.\n",
    "\n",
    "Steps: 2. - 4.\n",
    "- flatten upper triangular and store DataFrames in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Execute only once, to preserve memory\n",
    "    corr_list = []\n",
    "    count = 0\n",
    "    for i, path_npz in enumerate(df_par.loc[:, 'path']):\n",
    "\n",
    "        corrBC = np.load(path_npz + '/correlation_fct_BC.npz')['corrBC']\n",
    "        \n",
    "        # flatten the upper triangular and reduce precision\n",
    "        corrBC_flat = corrBC[np.triu_indices(n_grid)].astype(np.float32)\n",
    " \n",
    "        corr_list.append(corrBC_flat)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            count += 1\n",
    "            df_corr = pd.DataFrame(corr_list, columns=cols_x1x2)\n",
    "            print(count, df_corr.shape)\n",
    "            df_corr.to_pickle(f'data/df_corr_batch_{count:02d}.p')\n",
    "            corr_list = []\n",
    "    df_corr = pd.DataFrame(corr_list, columns=cols_x1x2)\n",
    "    count += 1\n",
    "    print(count, df_corr.shape)\n",
    "    df_corr.to_pickle(f'data/df_corr_batch_{count:02d}.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps: 5. - 7.\n",
    "\n",
    "- restart notebook, reload, concat and save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_list = [pd.read_pickle(f'data/df_corr_batch_{count:02d}.p') for count in range(1, 20)]\n",
    "\n",
    "df_corr = pd.concat(df_corr_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.to_pickle(f'data/df_corr.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce correlation DataFrame further with PCA\n",
    "\n",
    "1. Restart notebook and skip the above part to only load the df_corr.p into memory (otherwise it can happen that the kernel dies in the following, at least on my local laptop).\n",
    "2. Standardize data and apply PCA.\n",
    "3. Check that the applied PCA does not destroy relevant information.\n",
    "\n",
    "Start with step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_pickle('data/df_corr.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with step 2: Use Principle Component analysis to reduce the number of columns further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_pca = Pipeline([\n",
    "    ('std', StandardScaler()),\n",
    "    ('pca', PCA(n_components=100, random_state=42))\n",
    "])\n",
    "\n",
    "arr_corr = pipe_pca.fit_transform(df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results (steps 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.9999999945681466\n"
     ]
    }
   ],
   "source": [
    "expl_var = pipe_pca.named_steps['pca'].explained_variance_ratio_\n",
    "\n",
    "print('Explained variance:', sum(expl_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform back and invert Standardscaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_back_scaled = np.dot(arr_corr, pipe_pca.named_steps['pca'].components_)\n",
    "\n",
    "corr_backscaled = pipe_pca.named_steps['std'].inverse_transform(corr_back_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_corr_mat(corr_backscaled, index):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge df_corr and df_par"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_par.shape, df_corr.shape)\n",
    "df_main = pd.merge(\n",
    "    left=df_par,\n",
    "    right=df_corr,\n",
    "    how='inner',\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
