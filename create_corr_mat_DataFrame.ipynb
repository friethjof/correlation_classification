{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add correlation matrices to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load *df_parameter* DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gBB</th>\n",
       "      <th>gCC</th>\n",
       "      <th>gAB</th>\n",
       "      <th>gAC</th>\n",
       "      <th>gBC</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>data/batch_001/run_00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>data/batch_001/run_00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>data/batch_001/run_00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>data/batch_001/run_00004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>data/batch_001/run_00005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gBB  gCC  gAB  gAC  gBC                      path\n",
       "0 -1.0 -1.0 -1.0 -1.0 -1.0  data/batch_001/run_00001\n",
       "1 -1.0 -1.0 -1.0 -1.0 -0.8  data/batch_001/run_00002\n",
       "2 -1.0 -1.0 -1.0 -1.0 -0.6  data/batch_001/run_00003\n",
       "3 -1.0 -1.0 -1.0 -1.0 -0.4  data/batch_001/run_00004\n",
       "4 -1.0 -1.0 -1.0 -1.0 -0.2  data/batch_001/run_00005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18014, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_par = pd.read_pickle('df_parameter.p')\n",
    "display(df_par.head())\n",
    "df_par.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.loc[:, 'gBB'].unique()\n",
    "df_par.loc[:, 'gBC'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 200  # size of grid\n",
    "\n",
    "# create labels for columns, these correspond to the upper triangular of a matrix\n",
    "cols_x1x2 = [f\"x{j+1}_x{i+1}\" for i in range(n_grid) for j in range(i+1)]\n",
    "\n",
    "print(cols_x1x2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(df_par, gBB, gCC, gAB, gAC, gBC):\n",
    "    \"\"\"Get index corresponding to a specific choice for the interaction parameters gXY\n",
    "    \n",
    "    Args:\n",
    "        df_par (DataFrame): DataFrame of the parameter space\n",
    "        gBB (float): interaction strength between B-B\n",
    "        gCC (float): interaction strength between C-C\n",
    "        gAB (float): interaction strength between A-B\n",
    "        gAC (float): interaction strength between A-C\n",
    "        gBC (float): interaction strength between B-C\n",
    "    \n",
    "    Returns:\n",
    "        index (int) corresponding to input interaction parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = (\n",
    "        (df_par.loc[:, 'gBB'] == gBB) &\n",
    "        (df_par.loc[:, 'gCC'] == gCC) &\n",
    "        (df_par.loc[:, 'gAB'] == gAB) &\n",
    "        (df_par.loc[:, 'gAC'] == gAC) &\n",
    "        (df_par.loc[:, 'gBC'] == gBC)\n",
    "    )\n",
    "    \n",
    "    if all(~mask):\n",
    "        print('No match found, return None')\n",
    "    elif mask.sum() == 1:\n",
    "        return np.where(mask)[0][0]\n",
    "    else:\n",
    "        print('More than one match found, return the first')\n",
    "        return np.where(mask)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load npz-files and add pixels as features\n",
    "\n",
    "1. Create new dataframe with flattened npz-matrices as rows and x1-x2 values as columns\n",
    "2. Concat parameter df and correlation-matrix df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt: Reading all at once (not working)\n",
    "\n",
    "Load npz-files and add pixels as features\n",
    "\n",
    "1. Create new dataframe with flattened npz-matrices as rows and x1-x2 values as columns\n",
    "2. Concat parameter df and correlation-matrix df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_list = []\n",
    "for i, path_npz in enumerate(df_par.loc[:, 'path']):\n",
    "    \n",
    "    corrBC = np.load(path_npz + '/correlation_fct_BC.npz')['corrBC']\n",
    "    corrBC_flat = corrBC.flatten().astype(np.float32)  # reduce precision to improve performance\n",
    "    corr_list.append(corrBC_flat)\n",
    "\n",
    "df_corr = pd.DataFrame(corr_list, columns=cols_x1x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attempt: Store as pickles and concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load npz files and change to float32.\n",
    "2. Correlation functions are symmetric, therefore, store only upper triangular.\n",
    "3. Flatten correlation matrices, they denote the rows of the DataFrames.\n",
    "4. Store DataFrames in chunks of 1000 as pickles.\n",
    "5. Restart notebook to drop loaded memory.\n",
    "6. Reload DataFrames and concat them to a big DataFrame.\n",
    "7. Store it as a pickle.\n",
    "\n",
    "1\\. First check if storing the matrices with float16-type is still acceptable or we should go with float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gBB, gCC, gAB, gAC, gBC = 1.0, 1.0, -1.0, 0.2, 0.2\n",
    "mask = (\n",
    "    (df_par.loc[:, 'gBB'] == gBB) &\n",
    "    (df_par.loc[:, 'gCC'] == gCC) &\n",
    "    (df_par.loc[:, 'gAB'] == gAB) &\n",
    "    (df_par.loc[:, 'gAC'] == gAC) &\n",
    "    (df_par.loc[:, 'gBC'] == gBC)\n",
    ")\n",
    "corrBC_example = np.load(df_par.loc[mask, 'path'].iloc[0] + '/correlation_fct_BC.npz')['corrBC']\n",
    "\n",
    "\n",
    "\n",
    "xgrid = np.linspace(-2.5, 2.5, 200)\n",
    "x, y = np.meshgrid(xgrid, xgrid)\n",
    "corrBC_float64 = np.array(corrBC_example, dtype=np.float64)\n",
    "corrBC_float32 = np.array(corrBC_example, dtype=np.float32)\n",
    "corrBC_float16 = np.array(corrBC_example, dtype=np.float16)\n",
    "\n",
    "print(np.abs(corrBC_float64 - corrBC_float32).max())\n",
    "print(np.abs(corrBC_float64 - corrBC_float16).max())\n",
    "print(np.abs(corrBC_float32 - corrBC_float16).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float32 and float16 seems to be fine. However, continue with float32.\n",
    "\n",
    "Steps: 2. - 4.\n",
    "- flatten upper triangular and store DataFrames in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Execute only once, to preserve memory\n",
    "    corr_list = []\n",
    "    count = 0\n",
    "    for i, path_npz in enumerate(df_par.loc[:, 'path']):\n",
    "\n",
    "        corrBC = np.load(path_npz + '/correlation_fct_BC.npz')['corrBC']\n",
    "        \n",
    "        # flatten the upper triangular and reduce precision\n",
    "        corrBC_flat = corrBC[np.triu_indices(n_grid)].astype(np.float32)\n",
    " \n",
    "        corr_list.append(corrBC_flat)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            count += 1\n",
    "            df_corr = pd.DataFrame(corr_list, columns=cols_x1x2)\n",
    "            print(count, df_corr.shape)\n",
    "            df_corr.to_pickle(f'data/df_corr_batch_{count:02d}.p')\n",
    "            corr_list = []\n",
    "    df_corr = pd.DataFrame(corr_list, columns=cols_x1x2)\n",
    "    count += 1\n",
    "    print(count, df_corr.shape)\n",
    "    df_corr.to_pickle(f'data/df_corr_batch_{count:02d}.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps: 5. - 7.\n",
    "\n",
    "- restart notebook, reload, concat and save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_list = [pd.read_pickle(f'data/df_corr_batch_{count:02d}.p') for count in range(1, 20)]\n",
    "\n",
    "df_corr = pd.concat(df_corr_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.to_pickle(f'data/df_corr.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce correlation DataFrame further with PCA\n",
    "\n",
    "1. Restart notebook and skip the above part to only load the df_corr.p into memory (otherwise it can happen that the kernel dies in the following, at least on my local laptop).\n",
    "2. Standardize data and apply PCA.\n",
    "3. Check that the applied PCA does not destroy relevant information.\n",
    "\n",
    "Start with step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_pickle('data/df_corr.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with step 2: Use Principle Component analysis to reduce the number of columns further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 10\n",
    "\n",
    "pipe_pca = Pipeline([\n",
    "    ('std', StandardScaler()),\n",
    "    ('pca', PCA(n_components=n_comp, random_state=42))\n",
    "])\n",
    "\n",
    "arr_corr = pipe_pca.fit_transform(df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results (steps 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_var = pipe_pca.named_steps['pca'].explained_variance_ratio_\n",
    "\n",
    "print('Explained variance:', sum(expl_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance for n_comp=2:  0.8722\n",
    "\n",
    "Explained variance for n_comp=5:  0.9885\n",
    "\n",
    "Explained variance for n_comp=10: 0.9985\n",
    "\n",
    "Explained variance for n_comp=15: 0.9996\n",
    "\n",
    "Transform back and invert Standardscaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_corr: (18014, 10) principal components\n",
      "(10, 20100) pca coefficients\n"
     ]
    }
   ],
   "source": [
    "print('arr_corr:', arr_corr.shape, 'principal components')\n",
    "print(pipe_pca.named_steps['pca'].components_.shape, 'pca coefficients')\n",
    "\n",
    "\n",
    "corr_back_scaled = np.dot(arr_corr, pipe_pca.named_steps['pca'].components_)\n",
    "\n",
    "corr_backscaled = pipe_pca.named_steps['std'].inverse_transform(corr_back_scaled)\n",
    "\n",
    "# needed to reconstruct decomposed correlation matrices\n",
    "np.savez('corr_mat_backscaled.npz', corr_backscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_corr_mat(df_par, corr_backscaled, index, bool_save=False):\n",
    "    \"\"\"Reconstruct correlation matrix from the PCA and compare to the original one.\n",
    "        \n",
    "    Args:\n",
    "        df_par (DataFrame): DataFrame containing the paths to of original correlation matrices\n",
    "        corr_backscaled (numpy.array): Get matrix where rows are flattened correlation\n",
    "                                       matrices with only upper trinagualr entries being stored.\n",
    "        index (int): index of the individual correlation matrix\n",
    "        bool_save (boolean): determines whether to save the figure, default=False\n",
    "        \n",
    "    Return:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    corr_mat_ori = np.load(df_par.loc[index, \"path\"] + '/correlation_fct_BC.npz')['corrBC']\n",
    "\n",
    "    n_grid = corr_mat_ori.shape[0]\n",
    "    \n",
    "    \n",
    "    corr_flat_pca = corr_backscaled[index, :]\n",
    "\n",
    "    corr_mat_pca_half = np.zeros((n_grid, n_grid))\n",
    "    corr_mat_pca_half[np.triu_indices(n_grid)] = corr_flat_pca\n",
    "\n",
    "    corr_mat_pca = np.rot90(np.fliplr(corr_mat_pca_half)) + corr_mat_pca_half - np.diag(np.diag(corr_mat_pca_half))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #===========================================================================\n",
    "    # Define grid structure\n",
    "    plt.rc('text', usetex=True)\n",
    "    fig = plt.figure(figsize=(8, 3), dpi=300)\n",
    "\n",
    "    fig.subplots_adjust(left=0.15, right=0.89, top=0.92, bottom=0.12)\n",
    "\n",
    "\n",
    "    canv = gridspec.GridSpec(1, 2, width_ratios=[2.1, 1], wspace=0.4)\n",
    "    canv_left = gridspec.GridSpecFromSubplotSpec(\n",
    "        1, 2, canv[0, 0], width_ratios=[1, 1], wspace=0.1)\n",
    "\n",
    "    ax1 = plt.subplot(canv_left[0, 0], aspect=1)\n",
    "    ax2 = plt.subplot(canv_left[0, 1], aspect=1)    \n",
    "    ax3 = plt.subplot(canv[0, 1], aspect=1)\n",
    "\n",
    "    \n",
    "    #===========================================================================\n",
    "    # make plots\n",
    "    x_grid = np.linspace(-100, 100, n_grid)\n",
    "    \n",
    "    x, y = np.meshgrid(x_grid, x_grid)\n",
    "    ax1.set_title('$\\mathcal{C}_{\\mathrm{ori}}$')\n",
    "    ax2.set_title(r'$\\mathcal{C}_{\\mathrm{pca}}$'+ f', n={n_comp}')\n",
    "    ax3.set_title(r'$|\\mathcal{C}_{\\mathrm{pca}} - \\mathcal{C}_{\\mathrm{ori}}|$')\n",
    "    \n",
    "    vmin, vmax = corr_mat_ori.min(), corr_mat_ori.max()\n",
    "    im = ax1.pcolormesh(x, y, corr_mat_ori, shading='auto')\n",
    "    im.set_rasterized(True)\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes(\"right\", size=\"9%\", pad=0.05)\n",
    "    cax.set_axis_off()\n",
    "    \n",
    "    im = ax2.pcolormesh(x, y, corr_mat_pca, shading='auto', vmin=vmin, vmax=vmax)\n",
    "    im.set_rasterized(True)\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"9%\", pad=0.05)\n",
    "    cbar = fig.colorbar(im, cax=cax, extend='both')\n",
    "    \n",
    "    im = ax3.pcolormesh(x, y, np.abs(corr_mat_ori - corr_mat_pca), shading='auto', cmap='bwr')\n",
    "    im.set_rasterized(True)\n",
    "    divider = make_axes_locatable(ax3)\n",
    "    cax = divider.append_axes(\"right\", size=\"9%\", pad=0.05)\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    \n",
    "    ax1.set_xlabel(r'$x_1$')\n",
    "    ax2.set_xlabel(r'$x_1$')\n",
    "    ax1.set_ylabel(r'$x_2$')\n",
    "    ax3.set_xlabel(r'$x_1$')\n",
    "    ax2.set_yticklabels([])\n",
    "    \n",
    "    # make title\n",
    "    title_str = ''\n",
    "    title_str += r'$g_{BB}=' + str(df_par.loc[index, 'gBB']) + '$, '\n",
    "    title_str += r'$g_{CC}=' + str(df_par.loc[index, 'gCC']) + '$, '\n",
    "    title_str += r'$g_{AB}=' + str(df_par.loc[index, 'gAB']) + '$, '\n",
    "    title_str += r'$g_{AC}=' + str(df_par.loc[index, 'gAC']) + '$, '\n",
    "    title_str += r'$g_{BC}=' + str(df_par.loc[index, 'gBC']) + '$'\n",
    "    \n",
    "    ax2.annotate(\n",
    "        title_str,\n",
    "        xy=(0.6, 1.25),\n",
    "        xycoords='axes fraction',\n",
    "        ha='center',\n",
    "    )\n",
    "    \n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.set_xticks([-100, 0, 100])\n",
    "        ax.set_yticks([-100, 0, 100])\n",
    "        \n",
    "\n",
    "    if bool_save:\n",
    "        fig_name = f'figures/corr_ind{index}_pca_n_{n_comp}.pdf'\n",
    "        \n",
    "        plt.savefig(fig_name)\n",
    "        plt.close()\n",
    "\n",
    "        if fig_name[-3:] == 'png':\n",
    "            subprocess.check_output([\"convert\", fig_name, \"-trim\", fig_name])\n",
    "        elif fig_name[-3:] == 'pdf':\n",
    "            subprocess.check_output([\"pdfcrop\", fig_name, fig_name])\n",
    "\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_index(df_par, 0.2, 0.2, 0.2, 0.2, -0.2)\n",
    "\n",
    "plot_comparison_corr_mat(df_par, corr_backscaled, index=index, bool_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above analysis we find that 10 principal components can explain the variance to 99.85%.\n",
    "We proceed with this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge df_corr and df_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pca = pd.DataFrame(arr_corr, columns=[f'pca comp {i}' for i in range(1, n_comp+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gBB</th>\n",
       "      <th>gCC</th>\n",
       "      <th>gAB</th>\n",
       "      <th>gAC</th>\n",
       "      <th>gBC</th>\n",
       "      <th>path</th>\n",
       "      <th>pca comp 1</th>\n",
       "      <th>pca comp 2</th>\n",
       "      <th>pca comp 3</th>\n",
       "      <th>pca comp 4</th>\n",
       "      <th>pca comp 5</th>\n",
       "      <th>pca comp 6</th>\n",
       "      <th>pca comp 7</th>\n",
       "      <th>pca comp 8</th>\n",
       "      <th>pca comp 9</th>\n",
       "      <th>pca comp 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>data/batch_001/run_00001</td>\n",
       "      <td>-105.659465</td>\n",
       "      <td>37.965591</td>\n",
       "      <td>75.525219</td>\n",
       "      <td>0.440110</td>\n",
       "      <td>71.498446</td>\n",
       "      <td>-6.941616</td>\n",
       "      <td>-0.005033</td>\n",
       "      <td>24.897444</td>\n",
       "      <td>-19.683921</td>\n",
       "      <td>1.891975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>data/batch_001/run_00002</td>\n",
       "      <td>-102.790897</td>\n",
       "      <td>32.000799</td>\n",
       "      <td>59.039534</td>\n",
       "      <td>0.308656</td>\n",
       "      <td>49.503230</td>\n",
       "      <td>-9.023706</td>\n",
       "      <td>0.041075</td>\n",
       "      <td>15.930131</td>\n",
       "      <td>-10.015480</td>\n",
       "      <td>0.990542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>data/batch_001/run_00003</td>\n",
       "      <td>-98.020880</td>\n",
       "      <td>24.320418</td>\n",
       "      <td>41.510031</td>\n",
       "      <td>0.182524</td>\n",
       "      <td>29.262609</td>\n",
       "      <td>-9.871892</td>\n",
       "      <td>0.070699</td>\n",
       "      <td>8.940829</td>\n",
       "      <td>-2.802803</td>\n",
       "      <td>0.314344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>data/batch_001/run_00004</td>\n",
       "      <td>-90.632865</td>\n",
       "      <td>14.695513</td>\n",
       "      <td>23.230042</td>\n",
       "      <td>0.067592</td>\n",
       "      <td>11.804668</td>\n",
       "      <td>-9.495908</td>\n",
       "      <td>0.081263</td>\n",
       "      <td>4.117469</td>\n",
       "      <td>1.445592</td>\n",
       "      <td>-0.088344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>data/batch_001/run_00005</td>\n",
       "      <td>-79.346890</td>\n",
       "      <td>4.165145</td>\n",
       "      <td>4.233191</td>\n",
       "      <td>-0.034407</td>\n",
       "      <td>-0.536990</td>\n",
       "      <td>-8.301869</td>\n",
       "      <td>0.071124</td>\n",
       "      <td>1.586937</td>\n",
       "      <td>2.300852</td>\n",
       "      <td>-0.179525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gBB  gCC  gAB  gAC  gBC                      path  pca comp 1  pca comp 2  \\\n",
       "0 -1.0 -1.0 -1.0 -1.0 -1.0  data/batch_001/run_00001 -105.659465   37.965591   \n",
       "1 -1.0 -1.0 -1.0 -1.0 -0.8  data/batch_001/run_00002 -102.790897   32.000799   \n",
       "2 -1.0 -1.0 -1.0 -1.0 -0.6  data/batch_001/run_00003  -98.020880   24.320418   \n",
       "3 -1.0 -1.0 -1.0 -1.0 -0.4  data/batch_001/run_00004  -90.632865   14.695513   \n",
       "4 -1.0 -1.0 -1.0 -1.0 -0.2  data/batch_001/run_00005  -79.346890    4.165145   \n",
       "\n",
       "   pca comp 3  pca comp 4  pca comp 5  pca comp 6  pca comp 7  pca comp 8  \\\n",
       "0   75.525219    0.440110   71.498446   -6.941616   -0.005033   24.897444   \n",
       "1   59.039534    0.308656   49.503230   -9.023706    0.041075   15.930131   \n",
       "2   41.510031    0.182524   29.262609   -9.871892    0.070699    8.940829   \n",
       "3   23.230042    0.067592   11.804668   -9.495908    0.081263    4.117469   \n",
       "4    4.233191   -0.034407   -0.536990   -8.301869    0.071124    1.586937   \n",
       "\n",
       "   pca comp 9  pca comp 10  \n",
       "0  -19.683921     1.891975  \n",
       "1  -10.015480     0.990542  \n",
       "2   -2.802803     0.314344  \n",
       "3    1.445592    -0.088344  \n",
       "4    2.300852    -0.179525  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main = pd.merge(\n",
    "    left=df_par,\n",
    "    right=df_corr_pca,\n",
    "    how='inner',\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18014, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_pickle(f'df_main.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
